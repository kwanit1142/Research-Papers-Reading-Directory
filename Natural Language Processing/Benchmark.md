| S.No. | Research Paper | Architecture | Purpose | Link |
| ---- | ---- | ---- | ---- | ---- |
|1.|Attention augmentation with multi-residual in Bidirectional LSTM||Attention|[Link](https://github.com/kwanit1142/Research-Papers-Reading-Directory/blob/main/Natural%20Language%20Processing/Attention%20augmentation%20with%20multi-residual%20in%20Bidirectional%20LSTM.pdf)|
|2.|Deep contextualized word representations||Embeddings|[Link](https://github.com/kwanit1142/Research-Papers-Reading-Directory/blob/main/Natural%20Language%20Processing/Deep%20contextualized%20word%20representations.pdf)|
