| S.No. | Research Paper | Architecture/Purpose | Authors | Link |
| ---- | ---- | ---- | ---- | ---- |
|1.|Attention augmentation with multi-residual in Bidirectional LSTM|Attention|Ye Wang, Xinxiang Zhang, Mi Lua Han Wang, Yoonsuck Choe|[Link](https://github.com/kwanit1142/Research-Papers-Reading-Directory/blob/main/Natural%20Language%20Processing/Attention%20augmentation%20with%20multi-residual%20in%20Bidirectional%20LSTM.pdf)|
|2.|Deep contextualized word representations|Embeddings|Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer|[Link](https://github.com/kwanit1142/Research-Papers-Reading-Directory/blob/main/Natural%20Language%20Processing/Deep%20contextualized%20word%20representations.pdf)|
